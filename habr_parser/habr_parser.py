#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –ø–æ—Å—Ç–æ–≤ Habr –ø–æ –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É
–ê–≤—Ç–æ—Ä: Habr Parser v1.0
–î–∞—Ç–∞: 2025-10-23

–û–ø–∏—Å–∞–Ω–∏–µ:
–≠—Ç–æ—Ç –ø–∞—Ä—Å–µ—Ä –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ—Å—Ç–æ–≤ –Ω–∞ —Å–∞–π—Ç–µ Habr.com –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É.
–°–æ–±–∏—Ä–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç–µ–π, –∞–≤—Ç–æ—Ä–æ–≤, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–∫—Ü–∏–π, –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ JSON —Ñ–∞–π–ª.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
pip install requests beautifulsoup4 lxml

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
python habr_parser.py
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin
from datetime import datetime
from collections import Counter


class HabrParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ—Å—Ç–æ–≤ —Å —Å–∞–π—Ç–∞ Habr.com"""

    def __init__(self):
        self.base_url = "https://habr.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        self.all_posts_data = []

    def get_page_content(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ—É–¥–∞—á–Ω–∞ –¥–ª—è {url}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    return None

    def extract_search_results(self, search_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞"""
        content = self.get_page_content(search_url)
        if not content:
            return []

        soup = BeautifulSoup(content, 'html.parser')
        post_links = []

        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–æ—Å—Ç—ã
        selectors = [
            'h2.tm-title a',
            'h1.tm-title a',
            'a[href*="/articles/"]',
            'a[href*="/posts/"]',
        ]

        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href')
                if href:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in post_links:
                        post_links.append(full_url)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        unique_links = []
        for link in post_links:
            if ('/articles/' in link or '/posts/' in link) and link not in unique_links:
                unique_links.append(link)

        return unique_links

    def parse_number_with_suffix(self, text):
        """–ü–∞—Ä—Å–∏—Ç —á–∏—Å–ª–∞ —Å —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ (K, –∫, M, –º)"""
        if not text:
            return 0

        text = text.replace(',', '.').replace(' ', '')
        match = re.search(r'(\d+(?:\.\d+)?)\s*([Kk–ú–º]?)', text)
        if match:
            number = float(match.group(1))
            suffix = match.group(2).lower()
            if suffix in ['k', '–∫']:
                return int(number * 1000)
            elif suffix in ['m', '–º']:
                return int(number * 1000000)
            else:
                return int(number)
        return 0

    def extract_post_data(self, post_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ—Å—Ç–∞"""
        content = self.get_page_content(post_url)
        if not content:
            return None

        soup = BeautifulSoup(content, 'html.parser')
        post_data = {
            'url': post_url,
            'title': '',
            'author': '',
            'author_info': {},
            'content': '',
            'content_html': '',
            'views_count': 0,
            'reactions_count': 0,
            'comments_count': 0,
            'comments': [],
            'publication_date': '',
            'publication_time': '',
            'tags': [],
            'hubs': [],
            'reading_time': '',
            'difficulty': '',
            'company': ''
        }

        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = soup.select_one('h1.tm-title, h1.tm-article-title__title, h1')
            if title_elem:
                post_data['title'] = title_elem.get_text(strip=True)

            # –ê–≤—Ç–æ—Ä
            author_elem = soup.select_one('a.tm-user-info__username, .tm-user-info__user a')
            if author_elem:
                post_data['author'] = author_elem.get_text(strip=True)
                post_data['author_info']['profile_url'] = urljoin(self.base_url, author_elem.get('href', ''))

            # –ö–∞—Ä–º–∞ –∞–≤—Ç–æ—Ä–∞
            karma_elem = soup.select_one('.tm-user-info__stats-item_karma .tm-user-info__stats-counter')
            if karma_elem:
                post_data['author_info']['karma'] = karma_elem.get_text(strip=True)

            # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏
            content_elem = soup.select_one('.tm-article-body, .tm-article-presenter__body, .post__text')
            if content_elem:
                post_data['content'] = content_elem.get_text(separator='\n', strip=True)
                post_data['content_html'] = str(content_elem)

            # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
            views_elem = soup.select_one('.tm-icon-counter__value')
            if views_elem:
                views_text = views_elem.get_text(strip=True)
                post_data['views_count'] = self.parse_number_with_suffix(views_text)

            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = soup.select_one('.tm-votes-meter__value, .tm-vote__counter')
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                rating_match = re.search(r'[+-]?(\d+)', rating_text)
                if rating_match:
                    post_data['reactions_count'] = int(rating_match.group(1))

            # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            comments_elem = soup.select_one('a[href*="/comments/"], .tm-article-comments-counter')
            if comments_elem:
                comments_text = comments_elem.get_text(strip=True)
                comments_match = re.search(r'(\d+)', comments_text)
                if comments_match:
                    post_data['comments_count'] = int(comments_match.group(1))

            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            time_elem = soup.select_one('time')
            if time_elem:
                datetime_str = time_elem.get('datetime', '')
                if datetime_str:
                    post_data['publication_date'] = datetime_str
                    try:
                        dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                        post_data['publication_time'] = dt.strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        post_data['publication_time'] = time_elem.get_text(strip=True)

            # –¢–µ–≥–∏
            for tag_elem in soup.select('.tm-separated-list a, .tm-tags a'):
                tag_text = tag_elem.get_text(strip=True)
                if tag_text and tag_text not in post_data['tags']:
                    post_data['tags'].append(tag_text)

            # –•–∞–±—ã
            for hub_elem in soup.select('.tm-article-hubs a, .tm-hubs a'):
                hub_text = hub_elem.get_text(strip=True)
                if hub_text and hub_text not in post_data['hubs']:
                    post_data['hubs'].append(hub_text)

            # –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è
            reading_time_elem = soup.select_one('.tm-article-reading-time__label')
            if reading_time_elem:
                post_data['reading_time'] = reading_time_elem.get_text(strip=True)

            # –°–ª–æ–∂–Ω–æ—Å—Ç—å
            difficulty_elem = soup.select_one('.tm-article-complexity')
            if difficulty_elem:
                post_data['difficulty'] = difficulty_elem.get_text(strip=True)

            # –ö–æ–º–ø–∞–Ω–∏—è
            company_elem = soup.select_one('.tm-company-info__name a')
            if company_elem:
                post_data['company'] = company_elem.get_text(strip=True)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–∞: {e}")

        return post_data

    def extract_comments(self, post_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –ø–æ—Å—Ç—É"""
        if '/comments/' not in post_url:
            comments_url = post_url.rstrip('/') + '/comments/'
        else:
            comments_url = post_url

        content = self.get_page_content(comments_url)
        if not content:
            return []

        soup = BeautifulSoup(content, 'html.parser')
        comments = []

        try:
            comment_items = soup.select('.tm-comment-thread__comment, .tm-comment')

            for comment_item in comment_items:
                comment_data = {
                    'author': '',
                    'content': '',
                    'timestamp': '',
                    'rating': 0,
                    'level': 0
                }

                # –ê–≤—Ç–æ—Ä
                author_elem = comment_item.select_one('.tm-user-info__username, .tm-comment__username a')
                if author_elem:
                    comment_data['author'] = author_elem.get_text(strip=True)

                # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content_elem = comment_item.select_one('.tm-comment__body-content, .tm-comment-body')
                if content_elem:
                    comment_data['content'] = content_elem.get_text(separator='\n', strip=True)

                # –í—Ä–µ–º—è
                time_elem = comment_item.select_one('time')
                if time_elem:
                    comment_data['timestamp'] = time_elem.get('datetime', '')

                # –†–µ–π—Ç–∏–Ω–≥
                rating_elem = comment_item.select_one('.tm-comment-thread__comment-rating, .tm-votes-meter__value')
                if rating_elem:
                    rating_text = rating_elem.get_text(strip=True)
                    rating_match = re.search(r'[+-]?(\d+)', rating_text)
                    if rating_match:
                        comment_data['rating'] = int(rating_match.group(1))

                if comment_data['content'] or comment_data['author']:
                    comments.append(comment_data)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")

        return comments

    def parse_search_page(self, search_url, max_posts=None):
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞ –∏ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã"""
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {search_url}\n")

        post_links = self.extract_search_results(search_url)
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(post_links)} –ø–æ—Å—Ç–æ–≤\n")

        if not post_links:
            print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç—ã")
            return []

        if max_posts:
            post_links = post_links[:max_posts]
            print(f"‚ÑπÔ∏è  –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º—Å—è {max_posts} –ø–æ—Å—Ç–∞–º–∏\n")

        for i, post_url in enumerate(post_links, 1):
            print(f"[{i}/{len(post_links)}] {post_url}")

            post_data = self.extract_post_data(post_url)
            if post_data and post_data['title']:
                post_data['comments'] = self.extract_comments(post_url)
                self.all_posts_data.append(post_data)
                print(f"    ‚úì {post_data['title'][:60]}...")
                print(f"    üìù –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(post_data['comments'])}\n")
            else:
                print(f"    ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ\n")

            time.sleep(1)

        return self.all_posts_data

    def save_to_json(self, filename='habr_posts_data.json'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª"""
        try:
            output_data = {
                'metadata': {
                    'parsing_date': datetime.now().isoformat(),
                    'total_posts': len(self.all_posts_data),
                    'total_comments': sum(len(post['comments']) for post in self.all_posts_data),
                    'parser_version': '1.0'
                },
                'posts': self.all_posts_data
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filename}")
            print(f"   –ü–æ—Å—Ç–æ–≤: {len(self.all_posts_data)}")
            print(f"   –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {sum(len(post['comments']) for post in self.all_posts_data)}")

            return filename
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return None

    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if not self.all_posts_data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            return

        print("\n" + "="*60)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê")
        print("="*60)

        total_posts = len(self.all_posts_data)
        total_comments = sum(len(post['comments']) for post in self.all_posts_data)
        total_views = sum(post['views_count'] for post in self.all_posts_data)
        total_reactions = sum(post['reactions_count'] for post in self.all_posts_data)

        print(f"–ü–æ—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_posts}")
        print(f"–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å–æ–±—Ä–∞–Ω–æ: {total_comments}")
        print(f"–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –≤—Å–µ–≥–æ: {total_views:,}")
        print(f"–†–µ–∞–∫—Ü–∏–π –≤—Å–µ–≥–æ: {total_reactions}")

        if total_posts > 0:
            print(f"\n–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –ø–æ—Å—Ç:")
            print(f"  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {total_comments/total_posts:.1f}")
            print(f"  –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {total_views/total_posts:,.0f}")
            print(f"  –†–µ–∞–∫—Ü–∏–π: {total_reactions/total_posts:.1f}")

        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–≥–∏
        all_tags = []
        for post in self.all_posts_data:
            all_tags.extend(post['tags'])

        if all_tags:
            popular_tags = Counter(all_tags).most_common(5)
            print(f"\n–¢–æ–ø-5 —Ç–µ–≥–æ–≤:")
            for tag, count in popular_tags:
                print(f"  ‚Ä¢ {tag}: {count}")

        # –ê–∫—Ç–∏–≤–Ω—ã–µ –∞–≤—Ç–æ—Ä—ã
        authors = [post['author'] for post in self.all_posts_data if post['author']]
        if authors:
            popular_authors = Counter(authors).most_common(3)
            print(f"\n–¢–æ–ø-3 –∞–≤—Ç–æ—Ä–æ–≤:")
            for author, count in popular_authors:
                print(f"  ‚Ä¢ {author}: {count} –ø–æ—Å—Ç–æ–≤")

        print("="*60)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞
    search_url = "https://habr.com/ru/search/?q=–ø–æ–∏—Å–∫+—Ä–∞–±–æ—Ç—ã+&target_type=posts&order=relevance&hf=similar_posts_202412_B"

    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = HabrParser()

    # –ù–ê–°–¢–†–û–ô–ö–ò –ü–ê–†–°–ò–ù–ì–ê
    # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ max_posts=None –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤
    # –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ —á–∏—Å–ª–æ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, max_posts=10)
    MAX_POSTS = 40  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

    # –ü–∞—Ä—Å–∏–º
    parser.parse_search_page(search_url, max_posts=MAX_POSTS)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    parser.save_to_json('habr_posts_data.json')

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    parser.print_statistics()


if __name__ == "__main__":
    main()
